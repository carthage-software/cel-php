{"version":2,"defects":{"Tests\\Input\\InputTest::testLength":7,"Tests\\Input\\InputTest::testSliceInRange":7,"Tests\\Input\\InputTest::testConsumeWhiteSpace":7,"Tests\\Lexer\\LexerTest::testCursorPositionAndHasReachedEnd":7,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes raw prefix\"":7},"times":{"Tests\\Input\\InputTest::testIsEmpty":0.001,"Tests\\Input\\InputTest::testLength":0.002,"Tests\\Input\\InputTest::testCursorPosition":0,"Tests\\Input\\InputTest::testHasReachedEnd":0.001,"Tests\\Input\\InputTest::testSliceInRange":0.001,"Tests\\Input\\InputTest::testNext":0.001,"Tests\\Input\\InputTest::testSkip":0.001,"Tests\\Input\\InputTest::testConsume":0,"Tests\\Input\\InputTest::testConsumeRemaining":0,"Tests\\Input\\InputTest::testConsumeUntil":0,"Tests\\Input\\InputTest::testConsumeUntilIgnoreCase":0.001,"Tests\\Input\\InputTest::testConsumeThrough":0,"Tests\\Input\\InputTest::testConsumeWhiteSpace":0.001,"Tests\\Input\\InputTest::testRead":0,"Tests\\Input\\InputTest::testReadAt":0,"Tests\\Input\\InputTest::testReadAtOutOfBounds":0.001,"Tests\\Input\\InputTest::testIsAt":0,"Tests\\Input\\InputTest::testIsAtIgnoreCase":0,"Tests\\Input\\InputTest::testPeek":0,"Tests\\Input\\InputTest::testUtf8Characters":0.001,"Tests\\Input\\InputTest::testEdgeCases":0,"Tests\\Lexer\\LexerTest::testGetInput":0.001,"Tests\\Lexer\\LexerTest::testCursorPositionAndHasReachedEnd":0.001,"Tests\\Lexer\\LexerTest::testAdvanceWithEmptyInput":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"LeftParenthesis\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"RightParenthesis\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"LeftBracket\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"RightBracket\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"LeftBrace\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"RightBrace\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Dot\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Comma\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Colon\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Question\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Plus\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Minus\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Asterisk\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Slash\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Percent\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bang\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Less\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Greater\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"DoubleAmpersand\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"DoublePipe\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Equal\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"NotEqual\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"LessOrEqual\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"GreaterOrEqual\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String single quote\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String double quote\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String triple single quote\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String triple double quote\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String raw prefix\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String raw triple\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"String with escapes\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Unterminated string\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes simple\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes with hex escapes\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes with octal escapes\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes with unicode escapes\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes with mixed escapes\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Bytes raw prefix\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Integer decimal\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Integer negative\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Integer hex\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Integer octal\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Integer binary\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Unsigned integer\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Float simple\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Float negative\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Float no integer part\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Float with exponent\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Float with capital exponent\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Float with exponent and sign\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Boolean true\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Boolean false\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Null\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Identifier simple\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Identifier with underscore\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Identifier with numbers\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Keyword in\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Reserved word if\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Reserved word as\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Whitespace\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Single line comment\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Single line comment at EOF\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Unrecognized character\"":0,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Identifier and operator\"":0.001,"Tests\\Lexer\\LexerTest::testTokenization with data set \"Function call\"":0.001,"Tests\\Lexer\\LexerTest::testLosslessTokenization with data set \"Example from lexer.php\"":0,"Tests\\Lexer\\LexerTest::testLosslessTokenization with data set \"All features combined\"":0,"Tests\\Span\\SpanTest::testZero":0,"Tests\\Span\\SpanTest::testIsZero":0,"Tests\\Span\\SpanTest::testJoin":0,"Tests\\Span\\SpanTest::testHasOffset":0,"Tests\\Span\\SpanTest::testLength":0,"Tests\\Span\\SpanTest::testIsAfter":0,"Tests\\Span\\SpanTest::testIsBefore":0,"Tests\\Span\\SpanTest::testJsonSerialize":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #0":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #1":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #2":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #3":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #4":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #5":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #6":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #7":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #8":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #9":0.001,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #10":0,"Tests\\Token\\TokenKindTest::testIsDelimiter with data set #11":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #0":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #1":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #2":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #3":0.001,"Tests\\Token\\TokenKindTest::testIsOperator with data set #4":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #5":0.001,"Tests\\Token\\TokenKindTest::testIsOperator with data set #6":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #7":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #8":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #9":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #10":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #11":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #12":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #13":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #14":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #15":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #16":0,"Tests\\Token\\TokenKindTest::testIsOperator with data set #17":0.001,"Tests\\Token\\TokenKindTest::testIsWhitespace":0.001,"Tests\\Token\\TokenKindTest::testIsComment":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #0":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #1":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #2":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #3":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #4":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #5":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #6":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #7":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #8":0,"Tests\\Token\\TokenKindTest::testIsLiteral with data set #9":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #0":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #1":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #2":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #3":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #4":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #5":0,"Tests\\Token\\TokenKindTest::testIsKeyword with data set #6":0.001,"Tests\\Token\\TokenKindTest::testIsReserved with data set #0":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #1":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #2":0.001,"Tests\\Token\\TokenKindTest::testIsReserved with data set #3":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #4":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #5":0.001,"Tests\\Token\\TokenKindTest::testIsReserved with data set #6":0.001,"Tests\\Token\\TokenKindTest::testIsReserved with data set #7":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #8":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #9":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #10":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #11":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #12":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #13":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #14":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #15":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #16":0.001,"Tests\\Token\\TokenKindTest::testIsReserved with data set #17":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #18":0,"Tests\\Token\\TokenKindTest::testIsReserved with data set #19":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #0":0.001,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #1":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #2":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #3":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #4":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #5":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #6":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #7":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #8":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #9":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #10":0.001,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #11":0.001,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #12":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #13":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #14":0.001,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #15":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #16":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #17":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #18":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #19":0,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #20":0.001,"Tests\\Token\\TokenKindTest::testGetPrecedence with data set #21":0,"Tests\\Token\\TokenTest::testConstructorAndProperties":0.001}}